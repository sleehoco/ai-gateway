model_list:
  # Local Ollama Models (Windows Host)
  - model_name: "ollama/llama3.2:3b"
    litellm_params:
      model: "ollama/llama3.2:3b"
      api_base: "os.environ/OLLAMA_API_BASE"
      model_alias:
        "llama3.2:3b": "llama3.2:3b"

  - model_name: "ollama/llama3.2:1b"
    litellm_params:
      model: "ollama/llama3.2:1b"
      api_base: "os.environ/OLLAMA_API_BASE"
      model_alias:
        "llama3.2:1b": "llama3.2:1b"

  - model_name: "ollama/qwen2.5:3b"
    litellm_params:
      model: "ollama/qwen2.5:3b"
      api_base: "os.environ/OLLAMA_API_BASE"
      model_alias:
        "qwen2.5:3b": "qwen2.5:3b"

  # Uncensored Models (via Ollama)
  - model_name: "uncensored/dolphin-llama3.2:3b"
    litellm_params:
      model: "ollama/dolphin-llama3.2:3b"
      api_base: "os.environ/OLLAMA_API_BASE"
      model_alias:
        "dolphin-llama3.2:3b": "dolphin-llama3.2:3b"

  - model_name: "uncensored/dolphin-mistral:7b"
    litellm_params:
      model: "ollama/dolphin-mistral:7b"
      api_base: "os.environ/OLLAMA_API_BASE"
      model_alias:
        "dolphin-mistral:7b": "dolphin-mistral:7b"

  - model_name: "uncensored/dolphin-mixtral:8x7b"
    litellm_params:
      model: "ollama/dolphin-mixtral:8x7b"
      api_base: "os.environ/OLLAMA_API_BASE"
      model_alias:
        "dolphin-mixtral:8x7b": "dolphin-mixtral:8x7b"

  - model_name: "uncensored/nous-hermes2:7b"
    litellm_params:
      model: "ollama/nous-hermes2:7b"
      api_base: "os.environ/OLLAMA_API_BASE"
      model_alias:
        "nous-hermes2:7b": "nous-hermes2:7b"

  - model_name: "uncensored/eris:7b"
    litellm_params:
      model: "ollama/eris:7b"
      api_base: "os.environ/OLLAMA_API_BASE"
      model_alias:
        "eris:7b": "eris:7b"

  # OpenAI Models
  - model_name: "gpt-4o"
    litellm_params:
      model: "gpt-4o"
      api_key: "os.environ/OPENAI_API_KEY"

  - model_name: "gpt-4o-mini"
    litellm_params:
      model: "gpt-4o-mini"
      api_key: "os.environ/OPENAI_API_KEY"

  - model_name: "gpt-4-turbo"
    litellm_params:
      model: "gpt-4-turbo"
      api_key: "os.environ/OPENAI_API_KEY"

  # Anthropic Models
  - model_name: "claude-3-5-sonnet-20241022"
    litellm_params:
      model: "claude-3-5-sonnet-20241022"
      api_key: "os.environ/ANTHROPIC_API_KEY"

  - model_name: "claude-3-haiku-20240307"
    litellm_params:
      model: "claude-3-haiku-20240307"
      api_key: "os.environ/ANTHROPIC_API_KEY"

  # Groq Models
  - model_name: "groq/llama-3.1-8b-instant"
    litellm_params:
      model: "groq/llama-3.1-8b-instant"
      api_key: "os.environ/GROQ_API_KEY"

  - model_name: "groq/llama-3.1-70b-versatile"
    litellm_params:
      model: "groq/llama-3.1-70b-versatile"
      api_key: "os.environ/GROQ_API_KEY"

  - model_name: "groq/mixtral-8x7b-32768"
    litellm_params:
      model: "groq/mixtral-8x7b-32768"
      api_key: "os.environ/GROQ_API_KEY"

  # Google Gemini Models
  - model_name: "gemini/gemini-1.5-flash"
    litellm_params:
      model: "gemini/gemini-1.5-flash"
      api_key: "os.environ/GEMINI_API_KEY"

  - model_name: "gemini/gemini-1.5-pro"
    litellm_params:
      model: "gemini/gemini-1.5-pro"
      api_key: "os.environ/GEMINI_API_KEY"

  # Together AI Models
  - model_name: "together_ai/meta-llama/Llama-3.2-3B-Instruct-Turbo"
    litellm_params:
      model: "together_ai/meta-llama/Llama-3.2-3B-Instruct-Turbo"
      api_key: "os.environ/TOGETHER_AI_API_KEY"

  - model_name: "together_ai/meta-llama/Llama-3.1-8B-Instruct-Turbo"
    litellm_params:
      model: "together_ai/meta-llama/Llama-3.1-8B-Instruct-Turbo"
      api_key: "os.environ/TOGETHER_AI_API_KEY"

  - model_name: "together_ai/Qwen/Qwen2.5-7B-Instruct-Turbo"
    litellm_params:
      model: "together_ai/Qwen/Qwen2.5-7B-Instruct-Turbo"
      api_key: "os.environ/TOGETHER_AI_API_KEY"

# General Settings
general_settings:
  master_key: "os.environ/LITELLM_MASTER_KEY"
  database_url: "os.environ/DATABASE_URL"
  redis_url: "os.environ/REDIS_URL"
  success_callback: ["prometheus"]
  cache: true
  cache_type: "redis"
  redis_host: "redis"
  redis_port: 6379

# Router Settings with Model Groups
router:
  model_group_alias:
    # Local/uncensored groups
    "local-fast": ["ollama/llama3.2:1b", "ollama/llama3.2:3b"]
    "local-capable": ["ollama/llama3.2:3b", "ollama/qwen2.5:3b"]
    "uncensored-fast": ["uncensored/dolphin-llama3.2:3b", "uncensored/eris:7b"]
    "uncensored-capable": ["uncensored/dolphin-mistral:7b", "uncensored/nous-hermes2:7b"]
    "uncensored-premium": ["uncensored/dolphin-mixtral:8x7b"]
    "uncensored-all": ["uncensored/dolphin-mixtral:8x7b", "uncensored/dolphin-mistral:7b", "uncensored/dolphin-llama3.2:3b", "uncensored/nous-hermes2:7b", "uncensored/eris:7b"]
    
    # Cloud provider groups
    "openai-fast": ["gpt-4o-mini", "gpt-4-turbo"]
    "openai-capable": ["gpt-4o", "claude-3-5-sonnet-20241022"]
    "groq-fast": ["groq/llama-3.1-8b-instant", "groq/llama-3.1-70b-versatile"]
    "groq-capable": ["groq/mixtral-8x7b-32768"]
    "gemini-fast": ["gemini/gemini-1.5-flash"]
    "gemini-capable": ["gemini/gemini-1.5-pro"]
    "together-fast": ["together_ai/meta-llama/Llama-3.2-3B-Instruct-Turbo"]
    "together-capable": ["together_ai/meta-llama/Llama-3.1-8B-Instruct-Turbo"]
    
    # Mixed groups for cost optimization
    "fast-model": ["gpt-4o-mini", "ollama/llama3.2:3b", "groq/llama-3.1-8b-instant"]
    "capable-model": ["gpt-4o", "claude-3-5-sonnet-20241022", "uncensored/dolphin-mistral:7b"]
    "cost-effective": ["ollama/llama3.2:3b", "uncensored/dolphin-llama3.2:3b", "gpt-4o-mini"]
    "premium-model": ["gpt-4o", "claude-3-5-sonnet-20241022", "uncensored/dolphin-mixtral:8x7b", "groq/mixtral-8x7b-32768"]

# Fallback Configuration
litellm_settings:
  drop_params: ["stream", "logprobs", "top_logprobs"]
  set_verbose: true
  fallbacks: [
    {"model": "uncensored/dolphin-mixtral:8x7b", "fallbacks": ["uncensored/dolphin-mistral:7b", "uncensored/dolphin-llama3.2:3b"]},
    {"model": "gpt-4o", "fallbacks": ["claude-3-5-sonnet-20241022", "uncensored/dolphin-mistral:7b"]},
    {"model": "claude-3-5-sonnet-20241022", "fallbacks": ["gpt-4o", "uncensored/dolphin-mistral:7b"]},
    {"model": "groq/mixtral-8x7b-32768", "fallbacks": ["gpt-4o-mini", "uncensored/dolphin-mistral:7b"]},
    {"model": "gemini/gemini-1.5-pro", "fallbacks": ["gemini/gemini-1.5-flash", "gpt-4o-mini"]},
    {"model": "uncensored/dolphin-mistral:7b", "fallbacks": ["uncensored/dolphin-llama3.2:3b", "ollama/llama3.2:3b"]},
    {"model": "ollama/llama3.2:3b", "fallbacks": ["ollama/llama3.2:1b", "gpt-4o-mini"]}
  ]
  
  # Rate limiting per model type
  rate_limit: {
    "uncensored/*": {"requests_per_minute": 60, "tokens_per_minute": 10000},
    "openai/*": {"requests_per_minute": 100, "tokens_per_minute": 50000},
    "anthropic/*": {"requests_per_minute": 50, "tokens_per_minute": 40000},
    "groq/*": {"requests_per_minute": 120, "tokens_per_minute": 20000},
    "gemini/*": {"requests_per_minute": 60, "tokens_per_minute": 15000},
    "together_ai/*": {"requests_per_minute": 80, "tokens_per_minute": 12000},
    "ollama/*": {"requests_per_minute": 30, "tokens_per_minute": 8000}
  }